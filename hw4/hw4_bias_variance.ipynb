{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Bias/Variance, Overfitting, & Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE SURE TO RUN THIS CELL\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: For this homework, we will be doing a lot of polynomial regression. Please do not use existing polyfit tools such as those provided in scikit-learn or numpy unless otherwise noted. You need practice implementing it yourself by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I've provided 2 convenient plotting functions for you to use. `function_plot` will be used to plot the polynomials you're given/trying to estimate, while `scatter_plot` will be used to plot the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funtion_plot(f, featurize_fn=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "\n",
    "    x1 = np.arange(-1, 1, 0.1)\n",
    "    x2 = np.arange(-1, 1, 0.1)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    grid_points = np.stack((x1.flatten(), x2.flatten()), axis=1)\n",
    "    \n",
    "    if featurize_fn is not None:\n",
    "        grid_points = featurize_fn(grid_points)\n",
    "    y = f(grid_points)\n",
    "    y = y.reshape(x1.shape)\n",
    "\n",
    "    surf = ax.plot_surface(x1, x2, y, cmap=cm.coolwarm,\n",
    "                           linewidth=0, antialiased=False)\n",
    "\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(x, y, featurize_fn=None):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    \n",
    "    surf = ax.scatter(x[:,0], x[:,1], y, cmap=cm.coolwarm, c=y)\n",
    "\n",
    "    fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am defining a polynomial defined over 2 variables in `f_2d_polynomial`. This is the \"true\" function we will be trying to estimate. `f_2d_polynomial` takes in an Nx2 numpy array and returns a vector of length N, representing the value of the polynomial at each point. Note that I have slightly obfuscated the code so you can't just read off the polynomial coefficients. Obviously it is easy to reverse engineer, but these homeworks are provided in good faith for your learning experience.\n",
    "\n",
    "I've also defined a `generate_data` function which samples random points in the interval $[-1, 1]^2$, evaluates them using the true polynomial, and then adds gaussian noise to the output to represent measurement error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2d_polynomial(x):\n",
    "    x0, x1 = x[:,0], x[:,1]\n",
    "    secret = b'NCp4MSArIHgwKngxIC0gMTIqeDAqKjIgKyA4KngxKioyICsgNQ=='\n",
    "    y = eval(base64.b64decode(secret))\n",
    "    return y\n",
    "\n",
    "\n",
    "def generate_data(noise_strength, num_points):\n",
    "    x = np.random.rand(num_points, 2) * 2 - 1\n",
    "    y = f_2d_polynomial(x)\n",
    "    noise = np.random.randn(*y.shape)\n",
    "    return x, y + noise * noise_strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at what our polynomial truly looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funtion_plot(f_2d_polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! Feel free to move and inspect the plot with your mouse to get a better understanding of the polynomial.\n",
    "Now, let's sample some noisy training data and plot it to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(noise_strength=10, num_points=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! The data looks very noisy, but we can still make out the overall trend of the underlying function. Let's now try to recover the original function using this noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When given new data, your instinct should always be to try linear regression first. In this homework, we will start with linear regression and then build it into polynomial regression (estimating with a polynomial of degree $n$) by adding polynomial features to our data before performaing linear regression on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, perform ordinary linear regression on your 2-dimensional data points $x$. You are not allowed to call any external libraries or functions. Use only very basic numpy functions for your implementation. You may use `np.linalg.inv` if you wish.\n",
    "\n",
    "In this next codeblock, use `X_train` and `y_train` to create a function `estimated_f` that takes in new data points in a Nx2 array and returns a vector of N labels. Your resulting plot should look like a 2-dimensional line (plane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# estimated_f = ???\n",
    "\n",
    "funtion_plot(estimated_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our plot doesn't look like the true function at all. In this case, are we overfitting or underfitting? (Discuss how expressive our model is and how that factors into your answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of a line, let's estimate our function with a degree-2 polynomial. Note that this will make our model more expressive. How? Before, with a line, our model looked like this:\n",
    "\n",
    "$$ f_{hat}(x_1, x_2) = a_0 + a_1 * x_1 + a_2 * x2 $$\n",
    "\n",
    "If we want to use a 2d polynomial, our model will now look like this:\n",
    "\n",
    "$$ f_{hat}(x_1, x_2) = a_0 + a_1 * x_1 + a_2 * x2 + a_3 * x_1 * x_2 + a_4 * x_1^2 + a_5 * x_2^2$$\n",
    "\n",
    "Note that we've now included all terms and cross-terms up to degree 2. But this function is no longer linear in $x_1$ and $x_2$! This poses a challenge for us, since all we know how to do is linear regression. However, supposed someone gave us the higher order terms. What would our function look like then?:\n",
    "\n",
    "$$ f_{hat}(x_1, x_2, x_3, x_4, x_5) = a_0 + a_1 * x_1 + a_2 * x2 + a_3 * x_3 + a_4 * x_4 + a_5 * x_5$$\n",
    "where $x_3 = x_1 * x_2$, $x_4 = x_1^2$, $x_5 = x_2^2$.\n",
    "\n",
    "Now this function is linear in its inputs! And all we've done is use the existing features to create more features and add them to our dataset. This is exactly the idea behind polynomial regression - append higher-order polynomial terms as features to your dataset and then perform standard linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform 2d polynomial regression. You are not allowed to call any external libraries or functions. Use only very basic numpy functions for your implementation. You need to featurize your data appropriately by hand. You may use `np.linalg.inv` if you wish.\n",
    "\n",
    "In this next codeblock, use `X_train` and `y_train` to create 2 functions: `featurize_fn` that takes in data points in a Nx2 array and returns a Nxd array containing your featurized data (don't forget to add your bias term), and `estimated_f` which takes in new data in a Nxd array and returns a vector of N labels. Your resulting plot should look very close to the ground truth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# featurize_fn = ???\n",
    "# estimated_f = ???\n",
    "\n",
    "funtion_plot(estimated_f, featurize_fn=featurize_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice! Now let's see what the plot looks like when we include higher order terms up to degree ten. From now on, you may use `PolynomialFeatures` (imported below) to help aid constructing polynomial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# featurize_fn = ???\n",
    "# estimated_f = ???\n",
    "\n",
    "funtion_plot(f, featurize_fn=poly.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see clear signs of overfitting from this plot. Please describe whether you expect this function (degreee 10) or the previous one (degree 2) to perform better on new test data sampled from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Bias/Variance of Polynomial Degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm you answer, let's generate some test data and compare errors of models with varying degrees from degree 0 to degree 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = generate_data(noise_strength=10, num_points=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each degree from 0 to 15, compute the best fit polynomial based on the training data. You may use `PolynomialFeatures` for the feature computation, but the regression calculation must be done by hand. Then store the training error and test error respectively for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err, test_err = [], []\n",
    "\n",
    "for degree in range(15):\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of your train and test losses. X axis should be polynomial degree and y axis should be MSE. Be sure to include a labeled legened and label each axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot provides us a very clear picture of the bias/variance tradeoff in action. Note that while the train error only decreases with increasing polynomial degree, the test error starts to go back up beyond a certain point. Please explain why this phenomenon happens. Include in your explanation how bias and how variance affect (or don't affect) both train error and test error. Finally, give your recommendation for what degree you believe the polynomial to truly be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Bias/Variance of Extra Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if we can ameliorate the effects of overfitting by adding additional training data. For this section, we will fit a polynomial of degree 10. Fill in the code block below. We will train various models of degree 10, each with a different training dataset size from 1000 to 30000 in steps of 1000. To reduce variance in the model fitting, we will repeat each fit 20 times and take the average of their errors to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err, test_err = [], []\n",
    "\n",
    "for train_size in range(1000, 30000, 1000):\n",
    "    train_err_inner, test_err_inner = [], []\n",
    "    \n",
    "    for i in range(20):\n",
    "        X_train, y_train = generate_data(noise_strength=10, num_points=train_size)\n",
    "\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "    train_err.append(np.mean(train_err_inner))\n",
    "    test_err.append(np.mean(test_err_inner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an error plot similar to the one you made before. Make sure to label the axes properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, as the size of our training set increases, the measured overfitting effect decreases quite significantly (although with diminishing returns). Now, let's see this effect visually. In the codeblock below, fit a polynomial of degree 10 trained on 1k datapoints and visualize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(noise_strength=10, num_points=1000)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# featurize_fn = ???\n",
    "# estimated_f = ???\n",
    "\n",
    "funtion_plot(f, featurize_fn=poly.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, train a polynomial of degree 10 using 30k datapoints and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_data(noise_strength=10, num_points=30000)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# featurize_fn = ???\n",
    "# estimated_f = ???\n",
    "\n",
    "funtion_plot(f, featurize_fn=poly.fit_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why you think adding training data decreases the effect of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Final answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your observations so far, write down your estimate for the true polynomial function. You may train new models, playing with the degree of the polynomial and the number of training points to come up with your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "$ f(x_1, x_2) = $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
